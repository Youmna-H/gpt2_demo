{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Package installation\n",
    "These commands should be executed once at the beginning in order to install the packages required to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#package installation\n",
    "!pip install transformers\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Packages\n",
    "Run these comands in the beginning in order to import the different packages in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2TokenizerFast, GPT2LMHeadModel\n",
    "import random\n",
    "from transformers import set_seed\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model parameters\n",
    "You can set different parameters for the GPT-2 model as follows:\n",
    "1. 'model_name': GPT-2 comes in different sizes based on the number of parameters it was trained on:\n",
    "    - 'gpt2': 117M parameters\n",
    "    - 'gpt2-medium': 345M parameters\n",
    "    - 'gpt2-large': 774M parameters\n",
    "    - 'gpt2-xl': 1558M parameters\n",
    "\n",
    "    Note that the larger the model, the longer it takes to load and use for generation.\n",
    "\n",
    "2. 'num_samples': the number of sample responses to generate\n",
    "\n",
    "3. 'max_length': the maximum number of words to be generated in a response.\n",
    "\n",
    "4. 'sampling': The model generates the response word by word, conditioned on the input text and the sequence of words generated so far in the respose. There are different sampling strategies such as:\n",
    "    - 'top-p' (nucleas sampling)\n",
    "    - 'top-k' \n",
    "    - 'temperature'\n",
    "    \n",
    "    You can also combine different sampling approaches, but for simplicity, we will skip that. This is a nice blog post on sampling: https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "5. After choosing the 'sampling' strategy, you can set the corresponding value: 'top-p', 'top-k' or 'temperature', where 0 $\\leqslant$ top-p $\\leqslant$ 1, top-k $\\geqslant$ 0, and 0 $\\leqslant$ temperature $\\leqslant$ 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model_name': 'gpt2-medium',\n",
    "    'num_samples': 1,\n",
    "    'max_length': 100,\n",
    "    'sampling': 'top-p',\n",
    "    'top-p': 0.92,\n",
    "    'top-k': 40,\n",
    "    'temperature': 0.7,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading the model\n",
    "The following command loads the model into the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "set_seed(42)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(params['model_name'])\n",
    "model = GPT2LMHeadModel.from_pretrained(params['model_name'], pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Reading the input\n",
    "we can input the text that GPT-2 will response to either by command line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generation with interactive mode\n",
    "prompts = []\n",
    "prompt = input()\n",
    "prompts.append(prompt)\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or from a text file (input.txt) where each line contains a different prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating from a text file\n",
    "prompts = []\n",
    "with open('input.txt') as f:\n",
    "    for line in f:\n",
    "        prompt = line.strip()\n",
    "        if prompt == '':\n",
    "            continue\n",
    "        prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Generating the output\n",
    "By running this code you can generate the GPT-2 responses corresponding to the provided prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "==================================Sample 1 ==================================\n",
      "The team found that this strange human's words and language work by giving them the information that makes their voices sound like sounds that humans would make if spoken directly. Their findings have implications for future research on language, in which all spoken words must start with a syllable in order to sound like sounds.\n",
      "\n",
      "According to the research, the humans can learn languages faster than non-human primates, and that's a big deal for humans. \"\n",
      "********************************************************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generation itself\n",
    "output = {}\n",
    "for text in prompts:\n",
    "\tinput_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "\tmax_length = len(text.split()) + params['max_length']\n",
    "\tresponses = []\n",
    "\tif params['sampling'] == 'top-k':\n",
    "\t\tresponses = model.generate(input_ids, max_length = max_length, do_sample=True, \n",
    "            top_k=params['top-k'], num_return_sequences=params['num_samples'])\n",
    "\telif params['sampling'] == 'temperature':\n",
    "\t\tresponses = model.generate(input_ids, max_length = max_length, do_sample=True, \n",
    "            temperature=params['temperature'], num_return_sequences=params['num_samples'])\n",
    "    #default is nuclear (top-p)\n",
    "\telse:\n",
    "\t\tresponses = model.generate(input_ids, max_length = max_length, do_sample=True, \n",
    "            top_p=params['top-p'], num_return_sequences=params['num_samples'])\n",
    "# \tresponses = model.generate(input_ids, max_length=max_length)\n",
    "\tresponses = responses[:, input_ids.shape[-1]:]\n",
    "\tfinal_responses = []\n",
    "\toutput[text] = []\n",
    "\tfor i, r in enumerate(responses):\n",
    "\t\tresponse = tokenizer.decode(r, skip_special_tokens=True)#.strip().split('\\n')[0]\n",
    "\t\toutput[text].append(response)\n",
    "#df = pd.DataFrame(output)\n",
    "#df\n",
    "for k in output:\n",
    "    print('prompt: ' + k)\n",
    "    for i, r in enumerate(output[k]):\n",
    "        print('==================================Sample ' + str(i+1), '==================================')\n",
    "        print(r.strip())\n",
    "    print('********************************************************************************************************************')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
